{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "t2t_chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vQkysEvsf5jb"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brunotech/BioBERTpt/blob/master/meena_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbHaLsYbOIdY"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwhNO5KHS9CX"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "!pip install -q -U tensorflow-gpu==1.15.2\n",
        "!pip install -q -U tensorflow-datasets==3.2.1\n",
        "!pip install -q -U tensor2tensor\n",
        " \n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "project_dir = \"/gdrive/MyDrive/transformer-chatbot/\"\n",
        "MODEL_DIR = project_dir + \"saved_model/t2t_chatbot/\"\n",
        "DATASET_DIR = project_dir + \"conversational-dataset/\"\n",
        " \n",
        "!mkdir -p $DATASET_DIR\n",
        "!mkdir -p $MODEL_DIR\n",
        " \n",
        "tf.get_logger().propagate = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-VmGzKXDgbR"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRN4fw3bDfig"
      },
      "source": [
        "MAX_SAMPLES = 40000000\n",
        "DATA_DIR = MODEL_DIR + 'data'\n",
        "TMP_DIR = MODEL_DIR + 'tmp'\n",
        "TRAIN_DIR = MODEL_DIR + 'train'\n",
        "PROBLEM = 'chat_bot'\n",
        " \n",
        "USE_TPU = False\n",
        "MODEL = \"evolved_transformer\"\n",
        "HPARAMS = \"evolved_transformer_base\"\n",
        "NUM_ENCODER_LAYERS = 1\n",
        "NUM_DECODER_LAYERS = 12\n",
        "BATCH_SIZE = 4096\n",
        "MAX_LENGTH = 40\n",
        "VOCAB_SIZE = 2**13\n",
        "START_LEARNING_RATE = 0.01\n",
        " \n",
        "CONVERSATION_TURNS = 3\n",
        "\n",
        "TRAIN_STEPS = 300000 # Total number of train steps for all Epochs\n",
        "EVAL_STEPS = 100 # Number of steps to perform for each evaluation\n",
        "SAVE_CHECKPOINTS_STEPS = 5000\n",
        "KEEP_CHECKPOINT_MAX = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfValt_ONGRM"
      },
      "source": [
        "## Problem definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EjbmT4pVUZr"
      },
      "source": [
        "from tensor2tensor.data_generators import problem\n",
        "from tensor2tensor.data_generators import text_problems\n",
        "from tensor2tensor.utils import registry\n",
        "from tensor2tensor.data_generators import text_encoder\n",
        "from collections import deque\n",
        "import re\n",
        "\n",
        "PATH_TO_DATASET = DATASET_DIR + 'it'\n",
        "PATH_TO_PREPROCESSED = DATASET_DIR + \"preprocessed.txt\"\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r\"[^a-zA-Z0-9?.!,àèìòùáéíóú']+\", \" \", sentence)\n",
        "    sentence = sentence.replace(\" ' \", \" \")\n",
        "    sentence = sentence.strip()\n",
        "    return sentence\n",
        "\n",
        "if not os.path.isfile(PATH_TO_DATASET):\n",
        "    path_to_zip = tf.keras.utils.get_file(\n",
        "        DATASET_DIR + \"it.gz\",\n",
        "        origin='http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/mono/OpenSubtitles.it.gz')\n",
        "\n",
        "    !gzip -dk $path_to_zip\n",
        "\n",
        "if not os.path.isfile(PATH_TO_PREPROCESSED):\n",
        "    dataset_file = open(PATH_TO_DATASET, 'r')\n",
        "    preprocessed_file = open(PATH_TO_PREPROCESSED, 'w')\n",
        "    for i in range(MAX_SAMPLES):\n",
        "        line = dataset_file.readline()\n",
        "        if not line:\n",
        "            break\n",
        "        line = preprocess_sentence(line)\n",
        "        if line:\n",
        "            preprocessed_file.write(line + '\\n')\n",
        "    preprocessed_file.close()\n",
        "    dataset_file.close()\n",
        "else:\n",
        "    print(\"preprocessed dataset already exists\")\n",
        "        \n",
        "\n",
        "@registry.register_problem\n",
        "class ChatBot(text_problems.Text2TextProblem):\n",
        "    @property\n",
        "    def approx_vocab_size(self):\n",
        "        return VOCAB_SIZE\n",
        "    \n",
        "    @property\n",
        "    def is_generate_per_split(self):\n",
        "        return False\n",
        " \n",
        "    @property\n",
        "    def dataset_splits(self):\n",
        "        return [{\n",
        "            \"split\": problem.DatasetSplit.TRAIN,\n",
        "            \"shards\": 9,\n",
        "        }, {\n",
        "            \"split\": problem.DatasetSplit.EVAL,\n",
        "            \"shards\": 1,\n",
        "        }]\n",
        "\n",
        "    SENTENCE_SEPARATOR = \"<SEP>\"\n",
        "    SENTENCE_SEPARATOR_ID = 2\n",
        "\n",
        "    @property\n",
        "    def additional_reserved_tokens(self):\n",
        "        return [self.SENTENCE_SEPARATOR]\n",
        " \n",
        "    def generate_samples(self, data_dir, tmp_dir, dataset_split):\n",
        "        conversation = deque()\n",
        "        with open(PATH_TO_PREPROCESSED, 'r') as file:\n",
        "            conversation.append(file.readline().rstrip())\n",
        "            line = file.readline()\n",
        "            while line:\n",
        "                conversation.append(line.rstrip())\n",
        "                if len(conversation) > CONVERSATION_TURNS + 1:\n",
        "                    conversation.popleft()\n",
        "                yield {\n",
        "                    'inputs': list(conversation)[:-1], \n",
        "                    'targets': conversation[-1]\n",
        "                }\n",
        "                line = file.readline()\n",
        "\n",
        "    def generate_text_for_vocab(self, data_dir, tmp_dir):\n",
        "        with open(PATH_TO_PREPROCESSED, 'r') as file:\n",
        "            line = file.readline()\n",
        "            while line:\n",
        "                yield line.strip()\n",
        "                line = file.readline()\n",
        "\n",
        "    def generate_encoded_samples(self, data_dir, tmp_dir, dataset_split):\n",
        "\n",
        "        generator = self.generate_samples(data_dir, tmp_dir, dataset_split)\n",
        "        encoder = self.get_or_create_vocab(data_dir, tmp_dir)\n",
        "\n",
        "        def generate_encoded(generator, encoder):\n",
        "            count = 0\n",
        "            num_subwords_dataset = 0\n",
        "            for sample in generator:\n",
        "                encoded_inputs = []\n",
        "                for conversation_turn in sample[\"inputs\"]:\n",
        "                    encoded_inputs.extend(encoder.encode(conversation_turn))\n",
        "                    encoded_inputs.append(self.SENTENCE_SEPARATOR_ID)\n",
        "                encoded_inputs.pop()\n",
        "                encoded_inputs.append(text_encoder.EOS_ID)\n",
        "                if len(encoded_inputs) > MAX_LENGTH:\n",
        "                    encoded_inputs = encoded_inputs[-MAX_LENGTH:]\n",
        "                sample[\"inputs\"] = encoded_inputs\n",
        "                sample[\"targets\"] = encoder.encode(sample[\"targets\"])\n",
        "                sample[\"targets\"].append(text_encoder.EOS_ID)\n",
        "                # print some examples\n",
        "                if count > 100 and count < 110:\n",
        "                    print(\"_______INPUT_______\")\n",
        "                    print(encoder.decode(sample[\"inputs\"]))\n",
        "                    print(\"_______TARGET_______\")\n",
        "                    print(encoder.decode(sample[\"targets\"]))\n",
        "                count += 1\n",
        "                num_subwords_dataset += max(len(sample[\"inputs\"]), len(sample[\"targets\"]))\n",
        "                yield sample\n",
        "            print(f\"Num samples: {count}\")\n",
        "            print(f\"Tot number of subwords in the dataset: {num_subwords_dataset}\")\n",
        "\n",
        "        return generate_encoded(generator, encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIyjiL7UNTWb"
      },
      "source": [
        "## Generate data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVGPrUn1ebUj"
      },
      "source": [
        "from tensor2tensor import problems\n",
        "\n",
        "t2t_problem = problems.problem(PROBLEM)\n",
        "t2t_problem.generate_data(DATA_DIR, TMP_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlOFYJKQNWd8"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhdhrKFpZmaK"
      },
      "source": [
        "from tensor2tensor.utils.trainer_lib import create_run_config, create_experiment\n",
        "from tensor2tensor.utils.trainer_lib import create_hparams\n",
        "from tensor2tensor.utils import registry, hparam, learning_rate\n",
        "from tensor2tensor import models, problems\n",
        "import json\n",
        "\n",
        "# Init Hparams object from T2T Problem\n",
        "hparams = create_hparams(HPARAMS)\n",
        "\n",
        "# Make Changes to Hparams\n",
        "hparams.num_encoder_layers = NUM_ENCODER_LAYERS\n",
        "hparams.num_decoder_layers = NUM_DECODER_LAYERS\n",
        "hparams.batch_size = BATCH_SIZE\n",
        "hparams.max_length = MAX_LENGTH\n",
        "hparams.optimizer = 'Adafactor'\n",
        "hparams.learning_rate_constant = START_LEARNING_RATE\n",
        "hparams.learning_rate_warmup_steps = 10000\n",
        "hparams.learning_rate_schedule = \"constant*rsqrt_normalized_decay\"\n",
        "\n",
        "hparams_json = hparams.to_json()\n",
        "print(str(hparams_json))\n",
        "\n",
        "# Save hparams \n",
        "with open(MODEL_DIR + 'hparams.json', 'w') as json_file:\n",
        "    json_file.write(hparams_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS142dbdfugS"
      },
      "source": [
        "RUN_CONFIG = create_run_config(\n",
        "      model_dir=TRAIN_DIR,\n",
        "      model_name=MODEL,\n",
        "      save_checkpoints_steps = SAVE_CHECKPOINTS_STEPS,\n",
        "      keep_checkpoint_max = KEEP_CHECKPOINT_MAX\n",
        ")\n",
        "\n",
        "tensorflow_exp_fn = create_experiment(\n",
        "        run_config=RUN_CONFIG,\n",
        "        hparams=hparams,\n",
        "        model_name=MODEL,\n",
        "        problem_name=PROBLEM,\n",
        "        data_dir=DATA_DIR, \n",
        "        train_steps=TRAIN_STEPS, \n",
        "        eval_steps=EVAL_STEPS, \n",
        "        use_tpu=USE_TPU,\n",
        "        schedule=\"continuous_train_and_eval\",\n",
        "        eval_throttle_seconds=300,\n",
        "        use_xla=True # For acceleration\n",
        "    ) \n",
        "\n",
        "tensorflow_exp_fn.continuous_train_and_eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQkysEvsf5jb"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUG2xtDGdORG"
      },
      "source": [
        "# from tensor2tensor.utils.trainer_lib import create_run_config, create_experiment\n",
        "# from tensor2tensor.utils.trainer_lib import create_hparams\n",
        "# from tensor2tensor.utils import registry, hparam, learning_rate\n",
        "# from tensor2tensor import models, problems\n",
        "\n",
        "# hparams = create_hparams(HPARAMS)\n",
        "# hparams.num_encoder_layers = NUM_ENCODER_LAYERS\n",
        "# hparams.num_decoder_layers = NUM_DECODER_LAYERS\n",
        "# hparams.batch_size = BATCH_SIZE\n",
        "# hparams.max_length = MAX_LENGTH\n",
        "\n",
        "# RUN_CONFIG = create_run_config(\n",
        "#       model_dir=TRAIN_DIR,\n",
        "#       model_name=MODEL,\n",
        "#       save_checkpoints_steps = SAVE_CHECKPOINTS_STEPS,\n",
        "#       keep_checkpoint_max = KEEP_CHECKPOINT_MAX\n",
        "# )\n",
        "\n",
        "# tensorflow_exp_fn = create_experiment(\n",
        "#         run_config=RUN_CONFIG,\n",
        "#         hparams=hparams,\n",
        "#         model_name=MODEL,\n",
        "#         problem_name=PROBLEM,\n",
        "#         data_dir=DATA_DIR, \n",
        "#         train_steps=TRAIN_STEPS, \n",
        "#         eval_steps=1000, \n",
        "#         use_tpu=USE_TPU,\n",
        "#         schedule=\"evaluate\",\n",
        "#     ) \n",
        "\n",
        "# tensorflow_exp_fn.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGOC25N4dWdM"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jQ0uvnw-wff"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensor2tensor import models\n",
        "from tensor2tensor import problems\n",
        "from tensor2tensor.utils import hparams_lib\n",
        "from tensor2tensor.utils import registry\n",
        "from tensor2tensor.data_generators import text_problems\n",
        "import numpy as np\n",
        "import re\n",
        " \n",
        "# sampling parameters\n",
        "SAMPLING_TEMPERATURE = 0.88\n",
        "NUM_SAMPLES = 5\n",
        "MAX_LCS_RATIO = 0.8\n",
        " \n",
        "tfe = tf.contrib.eager\n",
        "tfe.enable_eager_execution()\n",
        "Modes = tf.estimator.ModeKeys\n",
        " \n",
        "chat_bot_problem = problems.problem(\"chat_bot\")\n",
        "ckpt_path = tf.train.latest_checkpoint(TRAIN_DIR)\n",
        "encoders = chat_bot_problem.feature_encoders(DATA_DIR)\n",
        "hparams = hparams_lib.create_hparams_from_json(MODEL_DIR + 'hparams.json')\n",
        "hparams.data_dir = DATA_DIR\n",
        "hparams_lib.add_problem_hparams(hparams, \"chat_bot\")\n",
        "hparams.sampling_method = \"random\"\n",
        "hparams.sampling_temp = SAMPLING_TEMPERATURE\n",
        " \n",
        "chatbot_model = registry.model(MODEL)(hparams, Modes.PREDICT)\n",
        " \n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = sentence.replace(\"'\", \"' \")\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(r\"[^a-zA-Z0-9?.!,àèìòùáéíóú']+\", \" \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    return sentence\n",
        " \n",
        "def postprocess_sentence(sentence):\n",
        "    # remove space before punctuation\n",
        "    sentence = sentence.rstrip(\" .\")\n",
        "    return re.sub(r\"\\s+(\\W)\", r\"\\1\", sentence)\n",
        " \n",
        "def encode(conversation, output_str=None):\n",
        "    \"\"\"Input str to features dict, ready for inference\"\"\"\n",
        "    encoded_inputs = []\n",
        "    for conversation_turn in conversation:\n",
        "        encoded_inputs += encoders[\"inputs\"].encode(conversation_turn) + [2]\n",
        "    encoded_inputs.pop()\n",
        "    encoded_inputs += [1]\n",
        "    if len(encoded_inputs) > hparams.max_length:\n",
        "        encoded_inputs = encoded_inputs[-hparams.max_length:]\n",
        "    batch_inputs = tf.reshape(encoded_inputs, [1, -1, 1])  # Make it 3D.\n",
        "    return {\"inputs\": batch_inputs}\n",
        " \n",
        "def decode(integers):\n",
        "    \"\"\"List of ints to str\"\"\"\n",
        "    integers = list(np.squeeze(integers))\n",
        "    if 1 in integers:\n",
        "        integers = integers[:integers.index(1)]\n",
        "    decoded = encoders[\"inputs\"].decode(integers)\n",
        "    return postprocess_sentence(decoded)\n",
        " \n",
        "def lcs_ratio(context, predicted): \n",
        "    m = len(context) \n",
        "    n = len(predicted) \n",
        "    L = [[None]*(n + 1) for i in range(m + 1)] \n",
        "    for i in range(m + 1): \n",
        "        for j in range(n + 1): \n",
        "            if i == 0 or j == 0 : \n",
        "                L[i][j] = 0\n",
        "            elif context[i-1] == predicted[j-1]: \n",
        "                L[i][j] = L[i-1][j-1]+1\n",
        "            else: \n",
        "                L[i][j] = max(L[i-1][j], L[i][j-1]) \n",
        "    return L[m][n] / n\n",
        " \n",
        "def predict(conversation):\n",
        "    preprocessed = [preprocess_sentence(x) for x in conversation]\n",
        "    encoded_inputs = encode(preprocessed)\n",
        "    print(\"decoded input: \" + decode(encoded_inputs[\"inputs\"]))\n",
        "    with tfe.restore_variables_on_create(ckpt_path):\n",
        "        while True:\n",
        "            output_candidates = [chatbot_model.infer(encoded_inputs) for _ in range(NUM_SAMPLES)]\n",
        "            output_candidates.sort(key = lambda x: -float(x[\"scores\"]))\n",
        " \n",
        "            for x in output_candidates:\n",
        "                print(str(float(x[\"scores\"])) + \"\\t\" + decode(x[\"outputs\"]))\n",
        " \n",
        "            for candidate in output_candidates:\n",
        "                decoded = decode(candidate[\"outputs\"])\n",
        "                if lcs_ratio(\" \".join(preprocessed), decoded) < MAX_LCS_RATIO:\n",
        "                    return decoded\n",
        " \n",
        " \n",
        "conversation = []\n",
        "while True:\n",
        "    sentence = input(\"Input: \")\n",
        "    conversation.append(sentence)\n",
        "    while len(conversation) > CONVERSATION_TURNS: \n",
        "        conversation.pop(0)\n",
        "    response = predict(conversation)\n",
        "    conversation.append(response)\n",
        "    print(response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJcdhi0s7cx9"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKtXU6xt65Xj"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir $TRAIN_DIR"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}